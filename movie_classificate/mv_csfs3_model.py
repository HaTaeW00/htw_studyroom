import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class MovieGenreDataset(Dataset):
    """ì˜í™” ì¤„ê±°ë¦¬ ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # í…ìŠ¤íŠ¸ í† í°í™”
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class BertGenreClassifier(nn.Module):
    """BERT ê¸°ë°˜ ì¥ë¥´ ë¶„ë¥˜ ëª¨ë¸"""
    
    def __init__(self, n_classes, model_name='bert-base-multilingual-cased', dropout_rate=0.3):
        super(BertGenreClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout_rate)
        
        # ì¶”ê°€ ë ˆì´ì–´ë¡œ ì„±ëŠ¥ í–¥ìƒ
        self.pre_classifier = nn.Linear(self.bert.config.hidden_size, 512)
        self.classifier = nn.Linear(512, n_classes)
        self.relu = nn.ReLU()
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        
        # ì¶”ê°€ ë ˆì´ì–´ í†µê³¼
        pre_logits = self.pre_classifier(pooled_output)
        pre_logits = self.relu(pre_logits)
        pre_logits = self.dropout(pre_logits)
        
        return self.classifier(pre_logits)

class ImprovedMovieGenreClassifierTrainer:
    """ê°œì„ ëœ ì˜í™” ì¥ë¥´ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name='bert-base-multilingual-cased', max_length=512):
        print("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        self.model_name = model_name
        self.max_length = max_length
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.label_encoder = LabelEncoder()
        
        print(f"ğŸš€ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"ğŸ“± ëª¨ë¸: {model_name}")

    def load_train_data(self, train_file='improved_train_data.csv'):
        """í›ˆë ¨ ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì¤‘... ({train_file})")
        
        try:
            df = pd.read_csv(train_file, encoding='utf-8-sig')
            
            print(f"âœ… í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ")
            print(f"ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}")
            
            # ê²°ì¸¡ê°’ í™•ì¸ ë° ì²˜ë¦¬
            print(f"\nğŸ” ê²°ì¸¡ê°’ í™•ì¸:")
            missing_values = df[['ì¤„ê±°ë¦¬', 'ì¥ë¥´']].isnull().sum()
            print(missing_values)
            
            # ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±°
            df = df.dropna(subset=['ì¤„ê±°ë¦¬', 'ì¥ë¥´'])
            print(f"ğŸ“Š ê²°ì¸¡ê°’ ì œê±° í›„: {len(df)}ê°œ ìƒ˜í”Œ")
            
            # ë‹¤ì¤‘ ì¥ë¥´ ì²˜ë¦¬ (ì²« ë²ˆì§¸ ì¥ë¥´ë§Œ ì‚¬ìš©)
            df['ì¥ë¥´_ë‹¨ì¼'] = df['ì¥ë¥´'].apply(lambda x: x.split(',')[0].strip())
            
            # ì¥ë¥´ ë¶„í¬ í™•ì¸
            print(f"\nğŸ­ í›ˆë ¨ ë°ì´í„° ì¥ë¥´ ë¶„í¬:")
            genre_counts = df['ì¥ë¥´_ë‹¨ì¼'].value_counts()
            print(genre_counts)
            
            return df
            
        except FileNotFoundError:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_file}")
            print("ğŸ’¡ ë¨¼ì € ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
            print("   python improved_processed_data.py")
            return None
    
    def load_evaluation_data(self, eval_file='improved_evaluation_data.csv'):
        """í‰ê°€ ë°ì´í„° ë¡œë“œ"""
        print(f"\nğŸ“Š í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘... ({eval_file})")
        
        try:
            df = pd.read_csv(eval_file, encoding='utf-8-sig')
            
            print(f"âœ… í‰ê°€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ")
            
            # ê²°ì¸¡ê°’ í™•ì¸ ë° ì²˜ë¦¬
            df = df.dropna(subset=['ì¤„ê±°ë¦¬', 'ì¥ë¥´'])
            
            # ë‹¤ì¤‘ ì¥ë¥´ ì²˜ë¦¬ (ì²« ë²ˆì§¸ ì¥ë¥´ë§Œ ì‚¬ìš©)
            df['ì¥ë¥´_ë‹¨ì¼'] = df['ì¥ë¥´'].apply(lambda x: x.split(',')[0].strip())
            
            # ì¥ë¥´ ë¶„í¬ í™•ì¸
            print(f"ğŸ­ í‰ê°€ ë°ì´í„° ì¥ë¥´ ë¶„í¬:")
            genre_counts = df['ì¥ë¥´_ë‹¨ì¼'].value_counts()
            print(genre_counts)
            
            return df
            
        except FileNotFoundError:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {eval_file}")
            return None
    
    def prepare_train_data(self, train_df):
        """í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""
        print(f"\nğŸ“‚ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        # í…ìŠ¤íŠ¸ì™€ ë¼ë²¨ ì¶”ì¶œ
        texts = train_df['ì¤„ê±°ë¦¬'].tolist()
        labels = train_df['ì¥ë¥´_ë‹¨ì¼'].tolist()
        
        # ë¼ë²¨ ì¸ì½”ë”©
        encoded_labels = self.label_encoder.fit_transform(labels)
        
        print(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(texts)}ê°œ")
        print(f"ğŸ­ ì´ ì¥ë¥´ ìˆ˜: {len(self.label_encoder.classes_)}")
        
        # ì¥ë¥´ ë§¤í•‘ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ”¢ ì¥ë¥´ ë¼ë²¨ ë§¤í•‘:")
        for i, genre in enumerate(self.label_encoder.classes_):
            count = np.sum(np.array(encoded_labels) == i)
            print(f"  {i}: {genre} ({count}ê°œ)")
        
        return texts, encoded_labels
    
    def prepare_evaluation_data(self, eval_df):
        """í‰ê°€ ë°ì´í„° ì¤€ë¹„"""
        print(f"\nğŸ“‚ í‰ê°€ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        # í…ìŠ¤íŠ¸ì™€ ë¼ë²¨ ì¶”ì¶œ
        texts = eval_df['ì¤„ê±°ë¦¬'].tolist()
        labels = eval_df['ì¥ë¥´_ë‹¨ì¼'].tolist()
        
        # í›ˆë ¨ ë°ì´í„°ì—ì„œ í•™ìŠµí•œ ë¼ë²¨ ì¸ì½”ë” ì‚¬ìš©
        try:
            encoded_labels = self.label_encoder.transform(labels)
        except ValueError as e:
            print(f"âš ï¸ í‰ê°€ ë°ì´í„°ì— í›ˆë ¨ ì¤‘ ë³´ì§€ ëª»í•œ ì¥ë¥´ê°€ ìˆìŠµë‹ˆë‹¤: {e}")
            # ì•Œë ¤ì§„ ì¥ë¥´ë§Œ í•„í„°ë§
            valid_indices = []
            valid_labels = []
            valid_texts = []
            
            for i, label in enumerate(labels):
                if label in self.label_encoder.classes_:
                    valid_indices.append(i)
                    valid_labels.append(label)
                    valid_texts.append(texts[i])
            
            encoded_labels = self.label_encoder.transform(valid_labels)
            texts = valid_texts
            print(f"ğŸ”„ ì•Œë ¤ì§„ ì¥ë¥´ë§Œ ì‚¬ìš©: {len(texts)}ê°œ")
        
        print(f"âœ… í‰ê°€ ë°ì´í„°: {len(texts)}ê°œ")
        
        return texts, encoded_labels
    
    def create_data_loaders(self, train_texts, train_labels, eval_texts, eval_labels, batch_size=16):
        """ë°ì´í„° ë¡œë” ìƒì„±"""
        print(f"\nğŸ”„ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = MovieGenreDataset(
            train_texts, train_labels, self.tokenizer, self.max_length
        )
        eval_dataset = MovieGenreDataset(
            eval_texts, eval_labels, self.tokenizer, self.max_length
        )
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)
        
        print(f"âœ… í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
        print(f"âœ… í‰ê°€ ë°°ì¹˜ ìˆ˜: {len(eval_loader)}")
        
        return train_loader, eval_loader
    
    def initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print(f"\nğŸ¤– ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        n_classes = len(self.label_encoder.classes_)
        self.model = BertGenreClassifier(n_classes, self.model_name)
        self.model.to(self.device)
        
        print(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (í´ë˜ìŠ¤ ìˆ˜: {n_classes})")
        
        return self.model
    
    def compute_class_weights(self, train_labels):
        """í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        classes = np.unique(train_labels)
        class_weights = compute_class_weight(
            'balanced',
            classes=classes,
            y=train_labels
        )
        
        # PyTorch tensorë¡œ ë³€í™˜
        class_weights_tensor = torch.FloatTensor(class_weights).to(self.device)
        
        print(f"\nğŸ“Š í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜:")
        for i, weight in enumerate(class_weights):
            genre = self.label_encoder.classes_[i]
            print(f"  {genre}: {weight:.3f}")
        
        return class_weights_tensor
    
    def train_model(self, train_loader, eval_loader, train_labels, epochs=5, learning_rate=1e-5, weight_decay=0.01, warmup_ratio=0.1):
        """ëª¨ë¸ í›ˆë ¨"""
        print(f"\nğŸ‹ï¸ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        print(f"ğŸ“Š ì—í¬í¬: {epochs}, í•™ìŠµë¥ : {learning_rate}, ê°€ì¤‘ì¹˜ ê°ì†Œ: {weight_decay}")
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        class_weights = self.compute_class_weights(train_labels)
        
        # ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        optimizer = AdamW(
            self.model.parameters(), 
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        total_steps = len(train_loader) * epochs
        warmup_steps = int(total_steps * warmup_ratio)
        
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
        
        # ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì†ì‹¤ í•¨ìˆ˜
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        # í›ˆë ¨ ê¸°ë¡
        train_losses = []
        eval_accuracies = []
        
        for epoch in range(epochs):
            print(f"\nğŸ“š ì—í¬í¬ {epoch + 1}/{epochs}")
            
            # í›ˆë ¨ ëª¨ë“œ
            self.model.train()
            total_train_loss = 0
            
            train_pbar = tqdm(train_loader, desc=f"í›ˆë ¨ ì¤‘")
            for batch in train_pbar:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                optimizer.zero_grad()
                
                outputs = self.model(input_ids, attention_mask)
                loss = criterion(outputs, labels)
                
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                scheduler.step()
                
                total_train_loss += loss.item()
                train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})

                # ...existing code... (í˜„ì¬ íŒŒì¼ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  ì•„ë˜ ë‚´ìš©ì„ ì¶”ê°€)
            
            avg_train_loss = total_train_loss / len(train_loader)
            train_losses.append(avg_train_loss)
            
            # í‰ê°€
            eval_accuracy = self.evaluate_model(eval_loader)
            eval_accuracies.append(eval_accuracy)
            
            print(f"ğŸ“Š ì—í¬í¬ {epoch + 1} ê²°ê³¼:")
            print(f"  - í‰ê·  í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f}")
            print(f"  - í‰ê°€ ì •í™•ë„: {eval_accuracy:.4f}")
        
        # í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”
        self.plot_training_history(train_losses, eval_accuracies)
        
        return train_losses, eval_accuracies
    
    def evaluate_model(self, eval_loader):
        """ëª¨ë¸ í‰ê°€"""
        self.model.eval()
        predictions = []
        actual_labels = []
        
        with torch.no_grad():
            for batch in eval_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids, attention_mask)
                _, preds = torch.max(outputs, dim=1)
                
                predictions.extend(preds.cpu().tolist())
                actual_labels.extend(labels.cpu().tolist())
        
        accuracy = accuracy_score(actual_labels, predictions)
        return accuracy
    
    def detailed_evaluation(self, eval_loader):
        """ìƒì„¸ í‰ê°€ ë° ë¶„ì„"""
        print(f"\nğŸ“Š ìƒì„¸ í‰ê°€ ì‹¤í–‰ ì¤‘...")
        
        self.model.eval()
        predictions = []
        actual_labels = []
        
        with torch.no_grad():
            for batch in tqdm(eval_loader, desc="í‰ê°€ ì¤‘"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids, attention_mask)
                _, preds = torch.max(outputs, dim=1)
                
                predictions.extend(preds.cpu().tolist())
                actual_labels.extend(labels.cpu().tolist())
        
        # ì •í™•ë„ ê³„ì‚°
        accuracy = accuracy_score(actual_labels, predictions)
        print(f"ğŸ¯ ì „ì²´ ì •í™•ë„: {accuracy:.4f}")
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        target_names = self.label_encoder.classes_
        report = classification_report(
            actual_labels, predictions, 
            target_names=target_names, 
            output_dict=True
        )
        
        print(f"\nğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        print(classification_report(actual_labels, predictions, target_names=target_names))
        
        # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        self.plot_confusion_matrix(actual_labels, predictions, target_names)
        
        return accuracy, report
    
    def plot_training_history(self, train_losses, eval_accuracies):
        """í›ˆë ¨ ê³¼ì • ì‹œê°í™”"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # í›ˆë ¨ ì†ì‹¤
        ax1.plot(train_losses, 'b-', linewidth=2, label='Training Loss')
        ax1.set_title('Training Loss Over Epochs', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # í‰ê°€ ì •í™•ë„
        ax2.plot(eval_accuracies, 'r-', linewidth=2, label='Evaluation Accuracy')
        ax2.set_title('Evaluation Accuracy Over Epochs', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“ˆ í›ˆë ¨ ê³¼ì • ê·¸ë˜í”„ê°€ 'training_history.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def plot_confusion_matrix(self, actual_labels, predictions, target_names):
        """í˜¼ë™ í–‰ë ¬ ì‹œê°í™”"""
        cm = confusion_matrix(actual_labels, predictions)
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(
            cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=target_names, yticklabels=target_names
        )
        plt.title('Movie Genre Classification Confusion Matrix', fontsize=16, fontweight='bold')
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š í˜¼ë™ í–‰ë ¬ì´ 'confusion_matrix.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def save_model(self, model_path='movie_genre_bert_model.pth', 
                   tokenizer_path='movie_genre_tokenizer', 
                   label_encoder_path='label_encoder.pkl'):
        """ëª¨ë¸ ì €ì¥"""
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        
        # ëª¨ë¸ ì €ì¥
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'n_classes': len(self.label_encoder.classes_),
                'model_name': self.model_name,
                'max_length': self.max_length
            }
        }, model_path)
        
        # í† í¬ë‚˜ì´ì € ì €ì¥
        self.tokenizer.save_pretrained(tokenizer_path)
        
        # ë¼ë²¨ ì¸ì½”ë” ì €ì¥
        import pickle
        with open(label_encoder_path, 'wb') as f:
            pickle.dump(self.label_encoder, f)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
        print(f"  - ëª¨ë¸: {model_path}")
        print(f"  - í† í¬ë‚˜ì´ì €: {tokenizer_path}")
        print(f"  - ë¼ë²¨ ì¸ì½”ë”: {label_encoder_path}")
    
    def predict_text(self, text):
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡"""
        self.model.eval()
        
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            return_token_type_ids=False,  # ë¬¸ì¥ êµ¬ë¶„ IDëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            padding='max_length',
            truncation=True,
            return_attention_mask=True,  # ì–´í…ì…˜ ë§ˆìŠ¤í¬ í¬í•¨
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_ids, attention_mask)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs, dim=1)
        
        predicted_genre = self.label_encoder.inverse_transform([predicted.cpu().numpy()[0]])[0]
        confidence = probabilities[0][predicted].item()
        
        # ìƒìœ„ 3ê°œ ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜
        top3_probs, top3_indices = torch.topk(probabilities[0], 3)
        top3_genres = self.label_encoder.inverse_transform(top3_indices.cpu().numpy())
        
        return predicted_genre, confidence, list(zip(top3_genres, top3_probs.cpu().numpy()))

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¬ ì˜í™” ì¥ë¥´ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ (ë¶„ë¦¬ëœ ë°ì´í„°ì…‹)")
    print("=" * 60)
    
    # ì„¤ì •
    CONFIG = {
        'model_name': 'bert-base-multilingual-cased',
        'max_length': 512,
        'batch_size': 32,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
        'epochs': 50,       # ì—í¬í¬ ì¦ê°€
        'learning_rate': 2e-5,  # í•™ìŠµë¥  ì¦ê°€
        'weight_decay': 0.01,
        'warmup_ratio': 0.15   # ì›Œë°ì—… ë¹„ìœ¨ ì¦ê°€
    }
    
    print(f"âš™ï¸ ì„¤ì •:")
    for key, value in CONFIG.items():
        print(f"  {key}: {value}")
    
    try:
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        trainer = ImprovedMovieGenreClassifierTrainer(
            model_name=CONFIG['model_name'],
            max_length=CONFIG['max_length']
        )
        
        # í›ˆë ¨ ë°ì´í„° ë¡œë“œ
        train_df = trainer.load_train_data('improved_train_data.csv')
        if train_df is None:
            print("âŒ í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨.")
            return
        
        # í‰ê°€ ë°ì´í„° ë¡œë“œ
        eval_df = trainer.load_evaluation_data('improved_evaluation_data.csv')
        if eval_df is None:
            print("âŒ í‰ê°€ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨.")
            return
        
        # ë°ì´í„° ì¤€ë¹„
        train_texts, train_labels = trainer.prepare_train_data(train_df)
        eval_texts, eval_labels = trainer.prepare_evaluation_data(eval_df)
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        train_loader, eval_loader = trainer.create_data_loaders(
            train_texts, train_labels, eval_texts, eval_labels,
            batch_size=CONFIG['batch_size']
        )
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        model = trainer.initialize_model()
        
        # ëª¨ë¸ í›ˆë ¨
        train_losses, eval_accuracies = trainer.train_model(
            train_loader, eval_loader, train_labels,
            epochs=CONFIG['epochs'],
            learning_rate=CONFIG['learning_rate'],
            weight_decay=CONFIG['weight_decay'],
            warmup_ratio=CONFIG['warmup_ratio']
        )
        
        # ìƒì„¸ í‰ê°€
        final_accuracy, classification_report = trainer.detailed_evaluation(eval_loader)
        
        # ëª¨ë¸ ì €ì¥
        trainer.save_model()
        
        # ì˜ˆì¸¡ ì˜ˆì‹œ
        print(f"\nğŸ”® ì˜ˆì¸¡ ì˜ˆì‹œ:")
        sample_texts = [
            "ì£¼ì¸ê³µì´ ì•…ë§ˆì™€ ì‹¸ìš°ë©° ì„¸ìƒì„ êµ¬í•˜ëŠ” íŒíƒ€ì§€ ì•¡ì…˜ ì´ì•¼ê¸°",
            "ë‘ ë‚¨ë…€ê°€ ìš´ëª…ì ìœ¼ë¡œ ë§Œë‚˜ ì‚¬ë‘ì— ë¹ ì§€ëŠ” ë¡œë§¨í‹±í•œ ë©œë¡œë“œë¼ë§ˆ",
            "ë¯¸ë˜ì—ì„œ ì˜¨ ë¡œë´‡ì´ ì¸ë¥˜ë¥¼ ìœ„í˜‘í•˜ëŠ” SF ì•¡ì…˜ ìŠ¤ë¦´ëŸ¬",
            "ê°€ì¡±ì´ í•¨ê»˜ ëª¨í—˜ì„ ë– ë‚˜ëŠ” ë”°ëœ»í•œ ê°€ì¡± ì˜í™”",
            "ë¬´ì„œìš´ ê·€ì‹ ì´ ë‚˜íƒ€ë‚˜ëŠ” ê³µí¬ í˜¸ëŸ¬ ì˜í™”"
        ]
        
        for text in sample_texts:
            predicted_genre, confidence, top3 = trainer.predict_text(text)
            print(f"\n  ğŸ“ '{text[:40]}...'")
            print(f"     ğŸ¥‡ 1ìœ„: {predicted_genre} (ì‹ ë¢°ë„: {confidence:.3f})")
            print(f"     ğŸ“Š ìƒìœ„ 3ê°œ:")
            for i, (genre, prob) in enumerate(top3):
                print(f"        {i+1}. {genre}: {prob:.3f}")
        
        print(f"\nğŸ‰ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… ì •í™•ë„: {final_accuracy:.4f}")
        
        # ìµœì¢… ìš”ì•½
        print(f"\nğŸ“ˆ ìµœì¢… ìš”ì•½:")
        print(f"  í›ˆë ¨ ë°ì´í„°: {len(train_texts)}ê°œ")
        print(f"  í‰ê°€ ë°ì´í„°: {len(eval_texts)}ê°œ")
        print(f"  ì¥ë¥´ ìˆ˜: {len(trainer.label_encoder.classes_)}ê°œ")
        print(f"  ìµœê³  í‰ê°€ ì •í™•ë„: {max(eval_accuracies):.4f}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()